{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Service Chatbot Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import liberaries and conversation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasmjensen/anaconda3/envs/bds-chatbot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# utils...\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "import spacy #spacy for quick language prepro\n",
    "nlp = spacy.load('da_core_news_md') #instantiating Danish module\n",
    "\n",
    "from sklearn.pipeline import make_pipeline #pipeline creation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
    "from sklearn.linear_model import LogisticRegression #Logit model\n",
    "\n",
    "# For controlling warnings in the code\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will call the below code to ignore all selected warnings in the code for getting more clearn outputs\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A JSON file, like \"convo.json,\" can be useful for organizing conversation flows, storing responses, or defining rules for a customer service chatbot. It's a structured way to manage dialogue trees, intents, entities, and responses within a chatbot framework. However, whether you specifically need a file named \"convo.json\" depends on the framework or platform you're using to build your chatbot.\n",
    "\n",
    "Many chatbot development platforms or frameworks offer their own ways to structure conversations, sometimes using JSON or similar formats. These files help maintain and organize the chatbot's knowledge base, making it easier to update and manage conversations.\n",
    "\n",
    "If you're developing a chatbot from scratch or using a platform that requires such a file, creating a \"convo.json\" or a similar structured file for your customer service chatbot could be beneficial. It allows you to define the chatbot's responses, conversation paths, and logic in a clear, organized manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/tobiasmjensen/Documents/aau_bds/M3 - Semester Project/M3_Semester_Project/convo_snt_chatbot.json'\n",
    "data = json.load(open(filepath,'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepro(texts):\n",
    "  \"\"\"\n",
    "  takes in a list/iterable of texts\n",
    "  lowercases, normalizes text\n",
    "  \"\"\"\n",
    "\n",
    "  clean_container = []\n",
    "\n",
    "  for text in nlp.pipe(texts, disable=[\"parser\", \"ner\"]):\n",
    "\n",
    "    txt = [token.lemma_.lower() for token in text # lemmatize and lower\n",
    "          if token.is_alpha # remove numbers\n",
    "          and not token.is_punct] # remove punctoation\n",
    "\n",
    "    clean_container.append(\" \".join(txt))\n",
    "\n",
    "  return clean_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "\n",
    "for i in data['intents']:\n",
    "  l = len(i['patterns'])\n",
    "  tuples = list(zip(i['patterns'], l*[i['tag']]))\n",
    "  training.extend(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hej</td>\n",
       "      <td>hilsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HallÃ¸j</td>\n",
       "      <td>hilsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goddag</td>\n",
       "      <td>hilsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>God dag</td>\n",
       "      <td>hilsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GÃ¥r det godt?</td>\n",
       "      <td>svar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             txt   label\n",
       "0            Hej  hilsen\n",
       "1         HallÃ¸j  hilsen\n",
       "2         Goddag  hilsen\n",
       "3        God dag  hilsen\n",
       "4  GÃ¥r det godt?    svar"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.DataFrame(training, columns=['txt','label'])\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.txt_p = text_prepro(training_df.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_name = 'ðŸ’¬ SNT Bot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidfvectorizer&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidfvectorizer&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#instantiate models and \"bundle up as pipeline\"\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "cls = LogisticRegression()\n",
    "\n",
    "tfidf_pipe = make_pipeline(tfidf, cls)\n",
    "tfidf_pipe.fit(training_df.txt_p, training_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SÃ¥dan indtaster du din rabatkode eller dit gavekort'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_pipe.predict(['Hvordan indtaster jeg rabatkode?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_reply(txt):\n",
    "  clean_text = text_prepro([txt])\n",
    "  tag = tfidf_pipe.predict(clean_text)[0]\n",
    "  for intent in data['intents']:\n",
    "    if tag == intent[\"tag\"]:\n",
    "      return f\"{bot_name}: {random.choice(intent['responses'])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ðŸ’¬ SNT Bot: Rabatkoder og gavekort indtastes i samme felt. FÃ¸lg de fÃ¥ trin herunder for at finde feltet:\\n\\n1. NÃ¥r du har valgt det Ã¸nskede produkt, trykker du pÃ¥ indkÃ¸bskurven Ã¸verst i hÃ¸jre hjÃ¸rne\\n2. AcceptÃ©r sÃ¥ handelsbetingelserne og tryk 'GÃ¥ til betaling'\\n3. Herefter kommer du til siden 'Oplysninger', hvor du skal klikke pÃ¥ 'Vis ordreoversigt'\\n4. Du kan nu indtaste dit gavekort eller din rabatkode i feltet\\n\\nHvis du er i besiddelse af et fysisk gavekort i form af et kreditkort, kan du bruge dette online og i de danske butikker ved at indtaste cifrene undtagen de fÃ¸rste fem.\\n\\nBemÃ¦rk venligst: Det ikke er muligt at tilfÃ¸je en rabatkode til en ordre, hvis man bestiller noget, der i forvejen er nedsat (sÃ¦t- eller kvantumrabat).\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_reply('Hvordan bruger jeg en rabatkode?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ’¬ SNT Bot: Hvis du har store lÃ¥r eller lÃ¦gge plejer det, som udgangspunkt, ikke at vÃ¦re noget problem. Vi tilbyder nemlig bÃ¥de Slim og Regular fit.\\n\\nDu kan altid prÃ¸ve vores bukser pÃ¥ i nogle timer og derefter bestemme dig for, om du synes, at de har givet sig tilstrÃ¦kkeligt og sidder, som du Ã¸nsker det.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_reply('Kan jeg passe bukserne hvis jeg har store lÃ¥r?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec !!HELP!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tobiasmjensen/Documents/aau_bds/M3 - Semester Project/M3_Semester_Project/app_snt_chatbot.ipynb Cell 21\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tobiasmjensen/Documents/aau_bds/M3%20-%20Semester%20Project/M3_Semester_Project/app_snt_chatbot.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Train your Word2Vec model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tobiasmjensen/Documents/aau_bds/M3%20-%20Semester%20Project/M3_Semester_Project/app_snt_chatbot.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m corpus \u001b[39m=\u001b[39m [\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]  \u001b[39m# Your text corpus\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tobiasmjensen/Documents/aau_bds/M3%20-%20Semester%20Project/M3_Semester_Project/app_snt_chatbot.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m word2vec_model \u001b[39m=\u001b[39m Word2Vec(sentences\u001b[39m=\u001b[39;49mcorpus, vector_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, window\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, min_count\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-chatbot/lib/python3.11/site-packages/gensim/models/word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, total_examples\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count,\n\u001b[1;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words, epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs, start_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha,\n\u001b[1;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha, compute_loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-chatbot/lib/python3.11/site-packages/gensim/models/word2vec.py:491\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \n\u001b[1;32m    455\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \n\u001b[1;32m    489\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 491\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_vocab(\n\u001b[1;32m    492\u001b[0m     corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, progress_per\u001b[39m=\u001b[39;49mprogress_per, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[1;32m    493\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n\u001b[1;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words \u001b[39m=\u001b[39m total_words\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-chatbot/lib/python3.11/site-packages/gensim/models/word2vec.py:586\u001b[0m, in \u001b[0;36mWord2Vec.scan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[39mif\u001b[39;00m corpus_file:\n\u001b[1;32m    584\u001b[0m     corpus_iterable \u001b[39m=\u001b[39m LineSentence(corpus_file)\n\u001b[0;32m--> 586\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[1;32m    588\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    589\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcollected \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types from a corpus of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m raw words and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m sentences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    590\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_vocab), total_words, corpus_count\n\u001b[1;32m    591\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[39mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-chatbot/lib/python3.11/site-packages/gensim/models/word2vec.py:569\u001b[0m, in \u001b[0;36mWord2Vec._scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39mif\u001b[39;00m sentence_no \u001b[39m%\u001b[39m progress_per \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    565\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    566\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPROGRESS: at sentence #\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m, processed \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m words, keeping \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    567\u001b[0m         sentence_no, total_words, \u001b[39mlen\u001b[39m(vocab),\n\u001b[1;32m    568\u001b[0m     )\n\u001b[0;32m--> 569\u001b[0m \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m sentence:\n\u001b[1;32m    570\u001b[0m     vocab[word] \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m\n\u001b[1;32m    571\u001b[0m total_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sentence)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not iterable"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train your Word2Vec model\n",
    "corpus = [...]  # Your text corpus\n",
    "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate models and \"bundle up as pipeline\"\n",
    "\n",
    "w2v = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "cls = LogisticRegression()\n",
    "\n",
    "\n",
    "w2v_pipe = make_pipeline(w2v, cls)\n",
    "w2v_pipe.fit(training_df.txt_p, training_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'Word2Vec<vocab=12, vector_size=100, alpha=0.025>' (type <class 'gensim.models.word2vec.Word2Vec'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tobiasmjensen/Documents/aau_bds/M3 - Semester Project/M3_Semester_Project/app_snt_chatbot.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tobiasmjensen/Documents/aau_bds/M3%20-%20Semester%20Project/M3_Semester_Project/app_snt_chatbot.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipe_2 \u001b[39m=\u001b[39m make_pipeline(w2v, \u001b[39mcls\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tobiasmjensen/Documents/aau_bds/M3%20-%20Semester%20Project/M3_Semester_Project/app_snt_chatbot.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pipe_2\u001b[39m.\u001b[39;49mfit(training_df\u001b[39m.\u001b[39;49mtxt_p, training_df\u001b[39m.\u001b[39;49mlabel)\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-chatbot/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-chatbot/lib/python3.11/site-packages/sklearn/pipeline.py:423\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    422\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 423\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    424\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-chatbot/lib/python3.11/site-packages/sklearn/pipeline.py:357\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps):\n\u001b[1;32m    355\u001b[0m     \u001b[39m# shallow copy of steps - this should really be steps_\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps)\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_steps()\n\u001b[1;32m    358\u001b[0m     \u001b[39m# Setup the memory\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     memory \u001b[39m=\u001b[39m check_memory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory)\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-chatbot/lib/python3.11/site-packages/sklearn/pipeline.py:241\u001b[0m, in \u001b[0;36mPipeline._validate_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(t, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mhasattr\u001b[39m(t, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\n\u001b[1;32m    239\u001b[0m         t, \u001b[39m\"\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     ):\n\u001b[0;32m--> 241\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAll intermediate steps should be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtransformers and implement fit and transform \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor be the string \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (t, \u001b[39mtype\u001b[39m(t))\n\u001b[1;32m    246\u001b[0m         )\n\u001b[1;32m    248\u001b[0m \u001b[39m# We allow last estimator to be None as an identity transformation\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    250\u001b[0m     estimator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39mand\u001b[39;00m estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    253\u001b[0m ):\n",
      "\u001b[0;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'Word2Vec<vocab=12, vector_size=100, alpha=0.025>' (type <class 'gensim.models.word2vec.Word2Vec'>) doesn't"
     ]
    }
   ],
   "source": [
    "w2v_pipe = make_pipeline(w2v, cls)\n",
    "w2v_pipe.fit(training_df.txt_p, training_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SÃ¥dan indtaster du din rabatkode eller dit gavekort'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pipe.predict(['Hvordan indtaster jeg rabatkode?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_reply(txt):\n",
    "  clean_text = text_prepro([txt])\n",
    "  tag = w2v_pipe.predict(clean_text)[0]\n",
    "  for intent in data['intents']:\n",
    "    if tag == intent[\"tag\"]:\n",
    "      return f\"{bot_name}: {random.choice(intent['responses'])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ðŸ’¬ SNT Bot: Rabatkoder og gavekort indtastes i samme felt. FÃ¸lg de fÃ¥ trin herunder for at finde feltet:\\n\\n1. NÃ¥r du har valgt det Ã¸nskede produkt, trykker du pÃ¥ indkÃ¸bskurven Ã¸verst i hÃ¸jre hjÃ¸rne\\n2. AcceptÃ©r sÃ¥ handelsbetingelserne og tryk 'GÃ¥ til betaling'\\n3. Herefter kommer du til siden 'Oplysninger', hvor du skal klikke pÃ¥ 'Vis ordreoversigt'\\n4. Du kan nu indtaste dit gavekort eller din rabatkode i feltet\\n\\nHvis du er i besiddelse af et fysisk gavekort i form af et kreditkort, kan du bruge dette online og i de danske butikker ved at indtaste cifrene undtagen de fÃ¸rste fem.\\n\\nBemÃ¦rk venligst: Det ikke er muligt at tilfÃ¸je en rabatkode til en ordre, hvis man bestiller noget, der i forvejen er nedsat (sÃ¦t- eller kvantumrabat).\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_reply('Hvordan bruger jeg en rabatkode?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ’¬ SNT Bot: Hvis du har store lÃ¥r eller lÃ¦gge plejer det, som udgangspunkt, ikke at vÃ¦re noget problem. Vi tilbyder nemlig bÃ¥de Slim og Regular fit.\\n\\nDu kan altid prÃ¸ve vores bukser pÃ¥ i nogle timer og derefter bestemme dig for, om du synes, at de har givet sig tilstrÃ¦kkeligt og sidder, som du Ã¸nsker det.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_reply('Kan jeg passe bukserne hvis jeg har store lÃ¥r?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        # use the reply_da function to get the bot's message\n",
    "        bot_message = tfidf_reply(message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        time.sleep(2)\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bds-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
